import numpy as np
import pandas as pd
from collections import Counter
import re
import pickle
import xgboost as xgb
import itertools
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support
import time
import copy
import json
import os
import matplotlib.pyplot as plt
import argparse

TRAINED_MODEL_PATH = 'trained_models'
CLASSIFICATION_RESULTS_PATH = 'prediction_results/classification'
REGRESSION_RESULTS_PATH = 'prediction_results/regression'
DATASET_FILE_PATH = 'dataset/cve_dataset.csv'
WORD_EMBEDDING_FILE_PATH = 'word_embeddings/word2vec_vectors.pickle'


# Helper functions

def read_dataset():
    """ Reads cve dataset and returns its DataFrame """

    cve_dataset = pd.read_csv(DATASET_FILE_PATH)
    cve_dataset = cve_dataset[
        ['cve_id', 'description', 'description_cleaned', 'cvssV2_baseScore',
         'class']]
    return cve_dataset


def read_word_embeddings():
    """ Reads word embeddings and return a dictionary """

    with open(WORD_EMBEDDING_FILE_PATH, 'rb') as f:
        word2vec_vectors = pickle.load(f)

        return word2vec_vectors


def train_validation_test_split(cve_dataset):
    """ Splits cve_dataset into train, validation and test set.
    Args:
        cve_dataset

    Returns:
        train_set
        val_set
        test_set
    """

    train_set, test_set = train_test_split(cve_dataset, test_size=0.2,
                                           random_state=1773,
                                           stratify=cve_dataset['class'])
    train_set, val_set = train_test_split(train_set, test_size=0.125,
                                          random_state=1773,
                                          stratify=train_set['class'])

    return train_set, val_set, test_set


def get_vectors_for_dataset(word2vec_vectors, dataset, label_key="class"):
    """ Returns word embedding vectors for given datasets.
    Args:
        word2vec_vectors: word embeddings
        dataset: train, test or validation dataset
        label_key: 'class' for classification or 'cvssV2_baseScore' regression
    """

    # inner helper function
    def get_description_vector(description):
        vectors = []
        for word in description.split():
            if word in word2vec_vectors:
                vectors.append(word2vec_vectors[word])

        return np.mean(vectors, axis=0)

    X = np.array([get_description_vector(x) for x in
                  dataset['description_cleaned'].values])
    y = dataset[label_key].values

    return X, y


def get_parameters_for_task(task_name='classification'):
    """
    Returns a combination of classification parameters for a given task
    Args:
    task_name -- 'classification' or 'regression'
    Returns:
    combination of parameters
    """

    params_set = None

    if task_name == 'classification':
        params_set = {
            'max_depth': [6, 8],
            'learning_rate': [0.05, 0.1, 0.3],
            'subsample': [0.8, 1],
            'objective': ['multi:softmax'],
            'predictor': ['gpu_predictor'],
            'tree_method': ['gpu_hist'],
            'num_class': [4],
            'n_jobs': [4],
            'eval_metric': [['mlogloss', 'merror']],
            'num_round': [200],
            'silent': [True]
        }

    elif task_name == 'regression':
        params_set = {
            'max_depth': [6, 8],
            'learning_rate': [0.1, 0.05, 0.3],
            'subsample': [0.8, 1],
            'objective': ['reg:linear'],
            'predictor': ['gpu_predictor'],
            'tree_method': ['gpu_hist'],
            'n_jobs': [4],
            'eval_metric': [['mae', 'rmse']],
            'num_round': [400],
            'silent': [True]
        }

    if params_set is not None:
        parameters = [
            dict(zip(params_set.keys(), v)) for v in
            itertools.product(*map(params_set.get, params_set.keys()))
        ]

        return parameters

    else:
        return None


def run_training_task(x_train, y_train, x_val, y_val, task='classification'):
    """Run training task for given parameter sets
    Args:
        x_train: training set
        y_train: training labels
        x_val: validation set
        y_val: validation labels
        task: "classification" or "regression"
    Returns:
        run_results: List of run results according to parameters
    """

    if task not in ['classification', 'regression']:
        print('Please specify a valid task. "classification" or "regression"')
        return None

    run_results = []

    dtrain = xgb.DMatrix(x_train, label=y_train)
    dval = xgb.DMatrix(x_val, label=y_val)

    parameters = get_parameters_for_task(task)

    print('\nRunning Task: {}\n'.format(task))

    eval_metrics = []
    if task == 'classification':
        eval_metrics = ['mlogloss', 'merror']
    elif task == 'regression':
        eval_metrics = ['mae', 'rmse']

    print('{} parameters set.\n'.format(len(parameters)))

    for params in parameters:
        print('Params: ', params)
        print()
        start = time.time()
        run_config = copy.deepcopy(params)
        run_config['start_time'] = start

        eval_result = {}
        eval_list = [(dtrain, 'train'), (dval, 'validation')]

        model = xgb.train(params, dtrain, params['num_round'], eval_list,
                          verbose_eval=10,
                          callbacks=[
                              xgb.callback.record_evaluation(eval_result)])

        end = time.time()
        elapsed_time = end - start
        run_config['end_time'] = end
        run_config['elapsed_time'] = elapsed_time

        for metric in eval_metrics:
            run_config['train_' + metric] = round(
                eval_result['train'][metric][-1], 3)
            run_config['val_' + metric] = round(
                eval_result['validation'][metric][-1], 3)

        run_config['model'] = model

        print('\nCompleted on {:.2f} seconds.\n'.format(elapsed_time))

        run_results.append(run_config)

    return run_results


def save_run_results(run_results, task_name):
    """Saves run results of the task to a json file."""
    out_file = TRAINED_MODEL_PATH + "/" + "xgb_" + task_name + "_results.json"

    # remove model from run result objects

    _run_results = [{k: v for k, v in elem.items() if k != 'model'} for elem in
                    run_results]

    with open(out_file, 'w') as f:
        json.dump(_run_results, f)


def get_best_run(run_results, metric):
    """ Finds best run according to validation metric and returns """

    best_run = sorted(run_results, key=lambda x: x[metric], reverse=True)[-1]

    print((
          'Best result parameters: learning_rate: {}, max_depth: {}, subsample: {}'
          .format(best_run['learning_rate'], best_run['max_depth'],
                  best_run['subsample'])))

    return best_run


def classification_report(predictions):
    """Evaluates predictions and generates a report for classification
    Args:
        predictions: DataFrame of predictions having actual and predicted values
    Returns:
        predictions: predictions itself
        conf_matrix: confusion matrix
        results_by_class: precision, recall, f1 measures for classes
        scores: accuracy, precision, recall, f1 scores
    """

    acc = predictions.loc[predictions.actual == predictions.pred].shape[0] / \
          predictions.shape[0]

    clf_results = np.array(
        precision_recall_fscore_support(predictions["actual"],
                                        predictions["pred"]))
    results_by_class = pd.DataFrame(clf_results.T,
                                    columns=['precision', 'recall', 'f1-score',
                                             'support'],
                                    index=[0, 1, 2, 3]
                                    )

    avg_precision = np.sum(clf_results[0] * clf_results[3]) / np.sum(
        clf_results[3])
    avg_recall = np.sum(clf_results[1] * clf_results[3]) / np.sum(
        clf_results[3])
    f1_score = np.sum(clf_results[2] * clf_results[3]) / np.sum(clf_results[3])

    scores = {
        'acc': round(acc, 3),
        'avg_precision': round(avg_precision, 3),
        'avg_recall': round(avg_recall, 3),
        'f1_score': round(f1_score, 3)
    }

    conf_matrix = pd.DataFrame(
        confusion_matrix(predictions["actual"], predictions["pred"]),
        columns=[0, 1, 2, 3],
        index=[0, 1, 2, 3])

    print('\nConfusion Matrix: \n')
    print(conf_matrix)
    print('\nPrecision, Recall, F1-Scores for Classes\n')
    print(results_by_class)
    print('\nScores\n')
    print(scores)

    return predictions, scores, results_by_class, conf_matrix


def regression_report(predictions):
    """Evaluates predictions and generates a report for classification
    Args:
        predictions: DataFrame of predictions having actual and predicted values
    Returns:
        predictions: predictions itself
        scores: mae, mmre, mdmre, rmse, mape scores
    """

    predictions['abs_err'] = np.abs(
        predictions['actual'] - predictions['pred'])
    predictions['rel_err'] = predictions['abs_err'] / predictions['actual']
    predictions['squared_error'] = (predictions['actual'] - predictions[
        'pred']) ** 2

    mae = predictions['abs_err'].mean()
    mmre = predictions['rel_err'].mean()
    mdmre = predictions['rel_err'].median()
    rmse = np.sqrt(predictions['squared_error'].mean())
    mape = mmre * 100

    scores = {
        'mae': round(mae, 3),
        'mmre': round(mmre, 3),
        'mdmre': round(mdmre, 3),
        'rmse': round(rmse, 3),
        'mape': round(mape, 3)
    }

    print('\nScores\n')
    print(scores)

    return predictions, scores, None, None


def run_prediction(model, x_test, y_test, index, task, num_chunks=10):
    """ Runs Predictions for test set."""

    if task not in ['classification', 'regression']:
        print('Please specify a valid task. "classification" or "regression"')
        return None

    x_tests = np.array_split(x_test, num_chunks)
    y_tests = np.array_split(y_test, num_chunks)
    pred_list = []

    for i in range(num_chunks):
        dtest = xgb.DMatrix(x_tests[i], label=y_tests[i])
        pred = model.predict(dtest)
        pred_list.append(pred)

    y_pred = np.concatenate(pred_list, axis=0)

    predictions = pd.DataFrame(np.stack((y_test, y_pred)).T,
                               columns=["actual", "pred"],
                               index=index)

    if task == 'classification':
        return classification_report(predictions)
    elif task == 'regression':
        return regression_report(predictions)


def save_trained_model(model, file_name):
    model_file_path = TRAINED_MODEL_PATH + "/" + file_name + ".xgbmodel"
    model.save_model(model_file_path)


def save_model_summary(best_run, train_scores, test_scores, task, file_name):
    summary_file_path = TRAINED_MODEL_PATH + "/" + file_name + "_summary.json"

    run_summary = {
        'task': task,
        'model_file_name': file_name + '.xgbmodel',
        'learning_rate': best_run["learning_rate"],
        'max_depth': best_run["max_depth"]
    }

    if task == "classification":
        run_summary['train_mlogloss'] = best_run["train_mlogloss"]
        run_summary['train_merror'] = best_run["train_merror"]
        run_summary['val_mlogloss'] = best_run["val_mlogloss"]
        run_summary['val_merror'] = best_run["val_merror"]
    elif task == "regression":
        run_summary['train_rmse'] = best_run["train_rmse"]
        run_summary['train_mae'] = best_run["train_mae"]
        run_summary['val_rmse'] = best_run["val_rmse"]
        run_summary['val_mae'] = best_run["val_mae"]

    for key in train_scores:
        run_summary['train_' + key] = train_scores[key]

    for key in test_scores:
        run_summary['test_' + key] = test_scores[key]

    with open(summary_file_path, 'w') as f:
        json.dump(run_summary, f)

    return run_summary


def save_predictions(predictions, run_summary, results_by_class, conf_matrix,
                     task, file_name):
    """ Saves predictions and results to corresponding output folders
    Args:
        predictions: (DataFrame) predictions
        run_summary: (dict) contains run summary information such as loss values 
                     and prediction scores
        results_by_class: (DataFrame) Precision & Recall & F1 measures per class
        conf_matrix: (DataFrame) Confusion matrix
        task: (str) task name, classification or regression
        file_name: (str) used for naming output files
    """

    if task not in ['classification', 'regression']:
        return

    folder = CLASSIFICATION_RESULTS_PATH if task == 'classification' else REGRESSION_RESULTS_PATH
    out_file_prefix = folder + "/" + file_name

    predictions.to_csv(out_file_prefix + '_predictions.csv', index=True)

    if task == "classification":
        conf_matrix.to_csv(out_file_prefix + '_conf_matrix.csv', index=True)
        results_by_class.to_csv(out_file_prefix + "_class_results.csv",
                                index=True)

    if task == "regression":
        # Save boxplot or relative error
        predictions[['rel_err']].boxplot()
        plt.savefig(out_file_prefix + '_relative_errors.png')

    with open(out_file_prefix + "_scores.json", 'w') as f:
        json.dump(run_summary, f)

    print("Predictions are saved to {} folder".format(folder))


def get_pretrained_model(task, file_extension=".xgbmodel"):
    """Looks-up and finds pretrained model for given task
    Args:
        task (str): task name, classification or regression
        file_extension (str): Model file extension. 
    Returns:
        model: pre_trained model
        model_summary: model summary information dictionary
    """
    model_name = None
    model_file = None
    model_summary_file = None
    model = None
    model_summary = None

    for file in os.listdir(TRAINED_MODEL_PATH):
        if file.endswith(file_extension) and task in file:
            model_file = TRAINED_MODEL_PATH + "/" + file
            model_name = file.split('.')[0]

        if model_name is not None:
            if file.endswith(
                    ".json") and task + "_summary" in file and model_name in file:
                model_summary_file = TRAINED_MODEL_PATH + "/" + file

        if model_file is not None and model_summary_file is not None:
            break

    if model_file is not None:
        model = xgb.Booster({'n_jobs': 4})
        model.load_model(model_file)

    if model_summary_file is not None:
        with open(model_summary_file, "rb") as f:
            model_summary = json.load(f)

    return model, model_summary


def str2bool(v):
    """Converts a candidate string to bool
    Args:
        v (str)
    Returns:
        True or False according to input
    
    Source: https://stackoverflow.com/a/43357954
    """
    if v.lower() in ('yes', 'True', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'False', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')


if __name__ == '__main__':

    # Step 0: Pre-Training Step  
    # create output folders if they dont exist
    output_folders = [TRAINED_MODEL_PATH, CLASSIFICATION_RESULTS_PATH,
                      REGRESSION_RESULTS_PATH]
    for out_folder in output_folders:
        if not os.path.exists(out_folder):
            os.makedirs(out_folder)

    # Read options from command line
    parser = argparse.ArgumentParser()
    parser.add_argument('--use_pretrained', default=True, type=str2bool,
                        help="""True for using pretrained models, 
                            False otherwise. Default: True""")

    args = parser.parse_args()
    use_pretrained = args.use_pretrained

    # Step 1: Read Dataset
    cve_dataset = read_dataset()

    # Step 2: Read Pre-trained word embeddings
    word_embeddings = read_word_embeddings()

    # Step 3: Split cve_dataset to train-validation-test
    train_set, val_set, test_set = train_validation_test_split(cve_dataset)

    # Step 4: Training tasks
    for task in ["classification", "regression"]:

        label_key = "class" if task == "classification" else "cvssV2_baseScore"
        eval_metric = "val_mlogloss" if task == "classification" else "val_rmse"
        file_name = "xgb_" + task

        # Step 4.1: Prepare data, label pairs
        x_train, y_train = get_vectors_for_dataset(word_embeddings, train_set,
                                                   label_key=label_key)
        x_val, y_val = get_vectors_for_dataset(word_embeddings, val_set,
                                               label_key=label_key)
        x_test, y_test = get_vectors_for_dataset(word_embeddings, test_set,
                                                 label_key=label_key)

        model, model_summary = get_pretrained_model(task)

        # Train if model could not be found or use_pretrained set to True
        if use_pretrained is False or model is None:
            # Step 4.2: Run training task
            run_results = run_training_task(x_train, y_train, x_val, y_val,
                                            task=task)
            save_run_results(run_results, task)

            print('{} training task is completed.'.format(task))

            # Step 4.3: Find best run configuration
            best_run = get_best_run(run_results, eval_metric)
            model = best_run['model']

            # Step 4.4: Save trained model
            save_trained_model(model, file_name=file_name)

        # Otherwise, skip to prediction
        else:
            best_run = model_summary
            print('Found a pre-trained model with following configuration:\n')
            print(model_summary)
            print('\nSkipping to prediction step...')

        # Step 4.5: Prediction for training set
        print('\nPrediction results for training set\n')
        _, train_scores, _, _ = run_prediction(model, x_train, y_train,
                                               train_set.index, task=task,
                                               num_chunks=10)

        # Step 4.6: Prediction for test set
        print('\nPrediction results for test set\n')
        predictions, test_scores, results_by_class, conf_matrix = run_prediction(
            model, x_test, y_test, test_set.index, task=task, num_chunks=2)

        # Step 4.7: Save model summary
        run_summary = save_model_summary(best_run, train_scores, test_scores,
                                         task, file_name)

        # Step 4.8: Save Final predictions
        save_predictions(predictions, run_summary, results_by_class,
                         conf_matrix, task, file_name)
